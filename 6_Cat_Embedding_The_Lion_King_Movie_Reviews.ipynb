{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Embeddings on new features.\n",
    "### In this Section\n",
    "1. I have created features like pos_or_neg. i.e. In a review which words are occuring more times. Based on this I have classified reviews as p, n or neutral.\n",
    "2. Derived polarity from TextBlob and added it as a numerical feature.\n",
    "3. Embedded all the layers after preprocessing and applied 1D CNN and LSTM on the embedded set.\n",
    "\n",
    "Note\n",
    "- This file requires glove.6B.50d.txt to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('alreviews_df_3000.csv') ## Allreviews data with more positive and less negative reviews\n",
    "valid2 = pd.read_csv('alreviews_df_1100_validation.csv') ## Extra validation set to perform second validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['createDate','displayImageUrl','displayName','hasProfanity','hasSpoilers','isSuperReviewer','isVerified','rating','timeFromCreation','updateDate','primary_key'],axis=1,inplace=True)\n",
    "valid2.drop(['createDate','displayImageUrl','displayName','hasProfanity','hasSpoilers','isSuperReviewer','isVerified','rating','timeFromCreation','updateDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['sentiment'] = np.where((data1['score']>3.0),0,1)\n",
    "valid2['sentiment'] = np.where((valid2['score']>3.0),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop('score',axis=1,inplace=True)\n",
    "valid2.drop('score',inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    #re.compile(regex).search(subject) is equivalent to re.search(regex, subject).\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = re.sub(\"’\", \"'\", text)\n",
    "    expanded_text = contractions_pattern.sub(expand_match, expanded_text)\n",
    "\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the Reviews\n",
    "def clean_doc(doc):\n",
    "    # Removing contractions\n",
    "    doc = expand_contractions(doc)\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split(' ')\n",
    "    \n",
    "    # Converting into lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove special characters from each token\n",
    "    tokens = [re.sub(r\"[^a-zA-Z#\\s]\",'',i) for i in tokens]\n",
    "    tokens = [re.sub(r\"[\\r\\n]\",'',i) for i in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessig the reviews\n",
    "data1['modified_review'] = data1.review.apply(lambda x: ' '.join(clean_doc(x)))\n",
    "valid2['modified_review'] = valid2.review.apply(lambda x: ' '.join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words in all negative reviews 16772\n"
     ]
    }
   ],
   "source": [
    "# Collecting Words from negative reviews\n",
    "negative_reviews = data1.loc[data1['sentiment']== 1,'modified_review']\n",
    "\n",
    "negative_words = ' '.join(negative_reviews)\n",
    "negative_words = negative_words.split(' ')\n",
    "negative_words = np.array(negative_words)\n",
    "\n",
    "print('Total Number of words in all negative reviews',len(negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of words in all positive reviews 23041\n"
     ]
    }
   ],
   "source": [
    "positive_reviews = data1.loc[data1['sentiment']== 0,'modified_review']\n",
    "\n",
    "positive_words = ' '.join(positive_reviews)\n",
    "positive_words = positive_words.split(' ')\n",
    "positive_words = np.array(positive_words)\n",
    "\n",
    "print('Total Number of words in all positive reviews',len(positive_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus 39813\n"
     ]
    }
   ],
   "source": [
    "# All words in our corpus\n",
    "all_words = ' '.join(data1.modified_review)\n",
    "all_words = all_words.split(' ')\n",
    "all_words = np.array(all_words)\n",
    "print('Total number of words in corpus',len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving pos_or_neg\n",
    "- The idea here is to create a variable pos_or_neg based on the number of positive or negative words present in the review.\n",
    "- Positive and Negative words list is created from the number of times the word is appearing in positive or negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the word is occuring more in positive or negative reviews\n",
    "def word_check(x,positive1,negative1,neutral1):\n",
    "    if ((pd.Series(positive_words) == x).sum()) > ((pd.Series(negative_words)==x).sum()):\n",
    "        positive1.append(x)\n",
    "    elif ((pd.Series(positive_words) == x).sum()) < ((pd.Series(negative_words)==x).sum()):\n",
    "        negative1.append(x)\n",
    "    else:\n",
    "        neutral1.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "5        None\n",
       "6        None\n",
       "7        None\n",
       "8        None\n",
       "9        None\n",
       "10       None\n",
       "11       None\n",
       "12       None\n",
       "13       None\n",
       "14       None\n",
       "15       None\n",
       "16       None\n",
       "17       None\n",
       "18       None\n",
       "19       None\n",
       "20       None\n",
       "21       None\n",
       "22       None\n",
       "23       None\n",
       "24       None\n",
       "25       None\n",
       "26       None\n",
       "27       None\n",
       "28       None\n",
       "29       None\n",
       "         ... \n",
       "39783    None\n",
       "39784    None\n",
       "39785    None\n",
       "39786    None\n",
       "39787    None\n",
       "39788    None\n",
       "39789    None\n",
       "39790    None\n",
       "39791    None\n",
       "39792    None\n",
       "39793    None\n",
       "39794    None\n",
       "39795    None\n",
       "39796    None\n",
       "39797    None\n",
       "39798    None\n",
       "39799    None\n",
       "39800    None\n",
       "39801    None\n",
       "39802    None\n",
       "39803    None\n",
       "39804    None\n",
       "39805    None\n",
       "39806    None\n",
       "39807    None\n",
       "39808    None\n",
       "39809    None\n",
       "39810    None\n",
       "39811    None\n",
       "39812    None\n",
       "Length: 39813, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_in_positive=[]\n",
    "more_in_negative=[]\n",
    "neutral=[]\n",
    "\n",
    "pd.Series(all_words).apply(lambda x:word_check(x,more_in_positive,more_in_negative,neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words that are occuring more in positive reviews: 26612\n",
      "Number of unique words that are occuring more in negative reviews : 11643\n",
      "Number of unique words that are neutral : 1558\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words that are occuring more in positive reviews:',len(more_in_positive))\n",
    "print('Number of unique words that are occuring more in negative reviews :',len(more_in_negative))\n",
    "print('Number of unique words that are neutral :',len(neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a reviews is having more positive or negative words.\n",
    "def check_sent(sent):\n",
    "    p=[]\n",
    "    n=[]\n",
    "    neutral=[]\n",
    "    tokens = sent.split(' ')\n",
    "    for w in tokens:\n",
    "        if w in more_in_positive:\n",
    "            p.append(w)\n",
    "        elif w in more_in_negative:\n",
    "            n.append(w)\n",
    "        elif w in neutral:\n",
    "            neutral.append(w)\n",
    "    if (len(p)>len(n))&(len(p)>len(neutral)):\n",
    "        code='p'\n",
    "    elif (len(n)>len(p))&(len(n)>len(neutral)):\n",
    "        code='n'\n",
    "    elif (len(neutral)>len(p))&(len(neutral)>len(n)):\n",
    "        code='neutral'\n",
    "    elif len(n)==0 & len(p)==0:\n",
    "        code='neutral'\n",
    "    else:\n",
    "        code='neutral'\n",
    "    return(p,n,code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p          2468\n",
       "n          396 \n",
       "neutral    136 \n",
       "Name: pos_or_neg, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['pos_or_neg'] = data1.modified_review.apply(lambda x:check_sent(x)[2])\n",
    "data1['pos_or_neg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p          948\n",
       "n          92 \n",
       "neutral    60 \n",
       "Name: pos_or_neg, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid2['pos_or_neg'] = valid2.modified_review.apply(lambda x:check_sent(x)[2])\n",
    "valid2['pos_or_neg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_or_neg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>32</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>49</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>2089</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment      0    1\n",
       "pos_or_neg           \n",
       "n           32    364\n",
       "neutral     49    87 \n",
       "p           2089  379"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data1['pos_or_neg'],data1.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_or_neg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>29</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>762</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment     0    1\n",
       "pos_or_neg          \n",
       "n           29   63 \n",
       "neutral     34   26 \n",
       "p           762  186"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(valid2['pos_or_neg'],valid2.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>modified_review</th>\n",
       "      <th>pos_or_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Great movie to see with the kids.</td>\n",
       "      <td>0</td>\n",
       "      <td>great movie see kid</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                review  sentiment      modified_review  \\\n",
       "699  Great movie to see with the kids.  0          great movie see kid   \n",
       "\n",
       "    pos_or_neg  \n",
       "699  p          "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving features on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test-1566381431512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['modified_review'] = test.Review.apply(lambda x: ' '.join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['pos_or_neg'] = test.modified_review.apply(lambda x:check_sent(x)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>modified_review</th>\n",
       "      <th>pos_or_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Love the new version. We took our grandson with us. We al agreed it was wonderful from start to finish.</td>\n",
       "      <td>0</td>\n",
       "      <td>love new version took grandson al agreed wonderful start finish</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      review  \\\n",
       "383  Love the new version. We took our grandson with us. We al agreed it was wonderful from start to finish.   \n",
       "\n",
       "     sentiment  \\\n",
       "383  0           \n",
       "\n",
       "                                                     modified_review  \\\n",
       "383  love new version took grandson al agreed wonderful start finish   \n",
       "\n",
       "    pos_or_neg  \n",
       "383  p          "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid2.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Review</th>\n",
       "      <th>modified_review</th>\n",
       "      <th>pos_or_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>93560</td>\n",
       "      <td>Best movie I've ever seen in my entire life. All of the critics are stupid. And wrong! It was perfect. They added and changed just enough. Perfect!</td>\n",
       "      <td>best movie ever seen entire life critic stupid wrong perfect added changed enough perfect</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ReviewID  \\\n",
       "684  93560      \n",
       "\n",
       "                                                                                                                                                  Review  \\\n",
       "684  Best movie I've ever seen in my entire life. All of the critics are stupid. And wrong! It was perfect. They added and changed just enough. Perfect!   \n",
       "\n",
       "                                                                               modified_review  \\\n",
       "684  best movie ever seen entire life critic stupid wrong perfect added changed enough perfect   \n",
       "\n",
       "    pos_or_neg  \n",
       "684  p          "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving polarity\n",
    "- Using textblob I have derived a polarity rating on the reviews.\n",
    "- -1 being extremely negative and 1 being extremely positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "data1['polarity'] = data1['review'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "valid2['polarity'] = valid2['review'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "test['polarity'] = test['Review'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "\n",
    "data1['number_of_sentences'] = [len(sent_tokenize(i)) for i in data1.review]\n",
    "valid2['number_of_sentences'] = [len(sent_tokenize(i)) for i in valid2.review]\n",
    "test['number_of_sentences'] = [len(sent_tokenize(i)) for i in test.Review]\n",
    "\n",
    "data1['number_of_words'] = [len(word_tokenize(i)) for i in data1.modified_review]\n",
    "valid2['number_of_words'] = [len(word_tokenize(i)) for i in valid2.modified_review]\n",
    "test['number_of_words'] = [len(word_tokenize(i)) for i in test.modified_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['polarity'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>number_of_words</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>169</th>\n",
       "      <th>175</th>\n",
       "      <th>180</th>\n",
       "      <th>187</th>\n",
       "      <th>200</th>\n",
       "      <th>209</th>\n",
       "      <th>221</th>\n",
       "      <th>243</th>\n",
       "      <th>366</th>\n",
       "      <th>401</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>257</td>\n",
       "      <td>228</td>\n",
       "      <td>199</td>\n",
       "      <td>191</td>\n",
       "      <td>156</td>\n",
       "      <td>140</td>\n",
       "      <td>109</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "number_of_words  1    2    3    4    5    6    7    8    9  10 ...   169  175  \\\n",
       "sentiment                                                      ...              \n",
       "0                6  128  257  228  199  191  156  140  109  90 ...   0    0     \n",
       "1                1  31   50   48   50   48   43   38   35   35 ...   1    1     \n",
       "\n",
       "number_of_words  180  187  200  209  221  243  366  401  \n",
       "sentiment                                                \n",
       "0                1    1    0    0    0    0    0    1    \n",
       "1                0    0    3    1    1    1    1    0    \n",
       "\n",
       "[2 rows x 113 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(data1.sentiment,data1.number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>number_of_words</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>97</th>\n",
       "      <th>99</th>\n",
       "      <th>102</th>\n",
       "      <th>113</th>\n",
       "      <th>127</th>\n",
       "      <th>164</th>\n",
       "      <th>213</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>102</td>\n",
       "      <td>94</td>\n",
       "      <td>81</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "number_of_words  1   2    3   4   5   6   7   8   9  10 ...   89  91  92  97  \\\n",
       "sentiment                                               ...                    \n",
       "0                4  49  102  94  81  55  53  52  42  24 ...   1   0   1   0    \n",
       "1                0  8   19   14  18  8   11  11  17  8  ...   0   1   0   1    \n",
       "\n",
       "number_of_words  99  102  113  127  164  213  \n",
       "sentiment                                     \n",
       "0                0   1    2    0    0    0    \n",
       "1                1   0    1    1    1    1    \n",
       "\n",
       "[2 rows x 79 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(valid2.sentiment,valid2.number_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment', 'modified_review', 'pos_or_neg', 'polarity',\n",
       "       'number_of_sentences', 'number_of_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment', 'modified_review', 'pos_or_neg', 'polarity',\n",
       "       'number_of_sentences', 'number_of_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ReviewID', 'Review', 'modified_review', 'pos_or_neg', 'polarity',\n",
       "       'number_of_sentences', 'number_of_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = ['number_of_sentences','number_of_words']\n",
    "numerical_float = ['polarity']\n",
    "categorical =['pos_or_neg']\n",
    "string = ['modified_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in numerical:\n",
    "    test[num] = test[num].astype('int64')\n",
    "    \n",
    "for cat in categorical:\n",
    "    test[cat] = test[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_attr = ['pos_or_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attr = 'sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attr = data1.select_dtypes(['int64','float64']).columns\n",
    "numerical_df = data1[numerical_attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>number_of_sentences</th>\n",
       "      <th>number_of_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503125</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.288939</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.937500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  number_of_sentences  number_of_words\n",
       "0  0.503125  4.0                  21.0           \n",
       "1  0.288939  2.0                  14.0           \n",
       "2  0.937500  2.0                  2.0            \n",
       "3  0.850000  2.0                  8.0            \n",
       "4  0.500000  1.0                  3.0            "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_df=numerical_df.astype('float')\n",
    "numerical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Input,Embedding,Dense,Flatten,concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for i in categorical_attr:\n",
    "    data1[i] = le.fit_transform(data1[i])\n",
    "    valid2[i] = le.transform(valid2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_categorical_train, data_categorical_valid1, \\\n",
    "data_numerical_train, data_numerical_valid1, \\\n",
    "data_string_train, data_string_valid1, \\\n",
    "Y_train, Y_valid1 = train_test_split(data1[categorical_attr],\n",
    "                                   numerical_df,\n",
    "                                   data1[string],\n",
    "                                   data1[target_attr],\n",
    "                                   test_size=0.3, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_categorical_valid2 = valid2[categorical_attr]\n",
    "data_numerical_valid2 = valid2[numerical_attr]\n",
    "data_string_valid2 = valid2[string]\n",
    "Y_valid2 = valid2[target_attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of categorical variables\n",
    "#### Convert categorical attributes to numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder = onehotencoder.fit(data_categorical_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder_train = OneHotEncoder.transform(data_categorical_train).toarray()\n",
    "OneHotEncoder_valid1 = OneHotEncoder.transform(data_categorical_valid1).toarray()\n",
    "OneHotEncoder_valid2 = OneHotEncoder.transform(data_categorical_valid2).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Target variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scalar= MinMaxScaler()\n",
    "scaled_attr = Scalar.fit(data_numerical_train)\n",
    "scaled_attr_train= scaled_attr.transform(data_numerical_train)\n",
    "scaled_attr_valid1= scaled_attr.transform(data_numerical_valid1)\n",
    "scaled_attr_valid2= scaled_attr.transform(data_numerical_valid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack both numerical and Categorical feautures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 6)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.hstack((scaled_attr_train, OneHotEncoder_train))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid1 = np.hstack((scaled_attr_valid1, OneHotEncoder_valid1))\n",
    "X_valid1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1100, 6)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid2 = np.hstack((scaled_attr_valid2, OneHotEncoder_valid2))\n",
    "X_valid2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing of Text\n",
    "##### Preprocessing of Review Text\n",
    "##### Get the length of the text having maximum number of occurances\n",
    "##### Get the unique count of text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(data_string_train['modified_review'].apply(len),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5,    6,    7,    8,    9,   10,   11,   12,   13,   14,   15,\n",
       "         16,   17,   18,   19,   20,   21,   22,   23,   24,   25,   26,\n",
       "         27,   28,   29,   30,   31,   32,   33,   34,   35,   36,   37,\n",
       "         38,   39,   40,   41,   42,   43,   44,   45,   46,   47,   48,\n",
       "         49,   50,   51,   52,   53,   54,   55,   56,   57,   58,   59,\n",
       "         60,   61,   62,   63,   64,   65,   66,   67,   68,   69,   70,\n",
       "         71,   72,   73,   74,   75,   76,   77,   78,   79,   80,   81,\n",
       "         82,   83,   84,   85,   86,   87,   88,   89,   90,   91,   92,\n",
       "         93,   94,   95,   96,   97,   98,   99,  100,  101,  102,  103,\n",
       "        104,  105,  106,  107,  108,  109,  110,  111,  112,  113,  114,\n",
       "        115,  116,  117,  118,  119,  120,  121,  122,  123,  124,  125,\n",
       "        126,  127,  128,  129,  130,  131,  132,  133,  134,  135,  136,\n",
       "        137,  138,  139,  140,  141,  142,  145,  146,  147,  148,  149,\n",
       "        150,  151,  152,  153,  154,  155,  156,  157,  158,  159,  160,\n",
       "        161,  163,  164,  165,  166,  168,  169,  170,  171,  173,  174,\n",
       "        176,  177,  178,  179,  180,  182,  183,  184,  186,  187,  188,\n",
       "        189,  190,  191,  192,  193,  194,  195,  196,  197,  198,  199,\n",
       "        200,  201,  202,  203,  206,  207,  209,  210,  211,  212,  213,\n",
       "        214,  215,  217,  219,  220,  221,  223,  224,  226,  229,  230,\n",
       "        231,  233,  235,  236,  240,  244,  246,  250,  251,  252,  254,\n",
       "        255,  256,  260,  261,  267,  268,  269,  270,  271,  274,  276,\n",
       "        277,  278,  282,  284,  286,  287,  289,  297,  299,  300,  302,\n",
       "        303,  308,  309,  310,  311,  312,  315,  318,  324,  325,  326,\n",
       "        328,  329,  331,  332,  334,  336,  338,  340,  343,  350,  365,\n",
       "        369,  370,  371,  373,  379,  392,  398,  401,  402,  405,  412,\n",
       "        432,  433,  437,  464,  465,  473,  521,  531,  536,  544,  559,\n",
       "        567,  585,  592,  594,  622,  625,  634,  635,  667,  696,  741,\n",
       "        749,  756,  810,  821,  824,  825,  832,  833, 1157, 1188, 1215,\n",
       "       1287, 1296, 1336, 1502, 1688, 2459], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  1,  1,  5,  5, 14,  5, 18, 12, 28, 26, 30, 34, 28, 31, 34,\n",
       "       35, 22, 34, 37, 26, 30, 19, 31, 27, 26, 24, 30, 35, 23, 27, 19, 24,\n",
       "       17, 30, 29, 17, 26, 26, 27, 20, 17, 17, 14, 22, 21, 23, 15, 15, 19,\n",
       "       14,  7, 26, 16, 21, 19, 18, 14,  8, 18,  8,  7, 18, 11,  7, 10, 17,\n",
       "       14,  8, 14, 10, 12, 13, 11,  3, 15,  9, 10, 12,  6, 11,  4, 11, 11,\n",
       "       10, 10,  9,  7, 12,  6,  5,  9,  8, 12,  7,  8,  6,  4,  9,  9,  8,\n",
       "        8,  3,  8,  7,  7,  6,  5,  7,  3,  4,  2,  2,  2, 10,  7,  4,  5,\n",
       "        4,  4,  3,  2,  6,  3,  7,  3,  2,  5,  2,  3,  5,  2,  3,  3,  2,\n",
       "        5,  2,  6,  5,  3,  2,  4,  4,  3,  4,  2,  2,  1,  2,  3,  2,  4,\n",
       "        2,  2,  3,  2,  2,  3,  3,  3,  1,  2,  2,  1,  3,  4,  1,  3,  1,\n",
       "        1,  1,  2,  1,  2,  2,  2,  1,  7,  2,  1,  3,  1,  1,  2,  3,  2,\n",
       "        1,  2,  1,  2,  2,  3,  3,  2,  2,  1,  2,  2,  1,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  3,  2,  1,  3,  2,  1,  1,  1,  2,  1,  2,  1,  1,\n",
       "        1,  1,  1,  1,  1,  4,  1,  1,  1,  2,  1,  1,  1,  5,  1,  1,  1,\n",
       "        3,  3,  2,  1,  1,  1,  1,  1,  2,  1,  2,  1,  2,  2,  1,  3,  2,\n",
       "        1,  1,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  2,  1,  2,  1,\n",
       "        1,  1,  2,  1,  1,  1,  1,  2,  1,  1,  1,  1,  1,  1,  1,  2,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  2,  1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_text_count_length = list(counts_elements).index(max(counts_elements))\n",
    "REVIEW_TEXT_MAX_SEQUENCE_LENGTH = unique_elements[max_text_count_length]\n",
    "REVIEW_TEXT_MAX_SEQUENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3618 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token='None')\n",
    "tokenizer.fit_on_texts(data_string_train['modified_review'])\n",
    "review_text_train = tokenizer.texts_to_sequences(data_string_train['modified_review'])\n",
    "review_text_valid1 = tokenizer.texts_to_sequences(data_string_valid1['modified_review'])\n",
    "review_text_valid2 = tokenizer.texts_to_sequences(data_string_valid2['modified_review'])\n",
    "\n",
    "\n",
    "word_index_review_text = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index_review_text))\n",
    "NUM_WORDS_REVIEW_TEXT = len(word_index_review_text)+1\n",
    "\n",
    "review_text_seq_train = pad_sequences(review_text_train, maxlen=REVIEW_TEXT_MAX_SEQUENCE_LENGTH)\n",
    "review_text_seq_valid1 = pad_sequences(review_text_valid1, maxlen=REVIEW_TEXT_MAX_SEQUENCE_LENGTH)\n",
    "review_text_seq_valid2 = pad_sequences(review_text_valid2, maxlen=REVIEW_TEXT_MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-577470c01f65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the whole embedding into memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove.6B.50d.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.50d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "review_embedding_matrix = np.zeros((NUM_WORDS_REVIEW_TEXT,50))\n",
    "review_word_not_in_glove_count = 0\n",
    "review_word_not_in_glove =[]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        review_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        review_word_not_in_glove.append(word)\n",
    "        review_word_not_in_glove_count = review_word_not_in_glove_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_word_not_in_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_word_not_in_glove_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer for numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat_inputs = Input(shape=(X_train.shape[1],),name='num_cat_inputs')\n",
    "out_num_cat = Dense(64, activation='relu')(num_cat_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer for Review Text\n",
    "#### If there are more than one word in the training data which are not present in Glove then train the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import GlobalMaxPooling1D, AveragePooling1D,Conv1D,Dropout,Embedding, MaxPooling1D,GRU,LSTM,SpatialDropout1D,GlobalMaxPool1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_input= Input(shape=(REVIEW_TEXT_MAX_SEQUENCE_LENGTH,),name='review_text_input')\n",
    "if (review_word_not_in_glove_count<=1):\n",
    "    text_embed = Embedding(input_dim=NUM_WORDS_REVIEW_TEXT,output_dim=50,weights=[review_embedding_matrix],trainable=False)(review_text_input)\n",
    "    con1d = Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1,kernel_initializer='normal')(text_embed)\n",
    "else:\n",
    "    text_embed = Embedding(input_dim=NUM_WORDS_REVIEW_TEXT,output_dim=50,weights=[review_embedding_matrix],trainable=True)(review_text_input)\n",
    "    con1d = Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1,kernel_initializer='normal')(text_embed)\n",
    "\n",
    "review_out_text = Flatten()(con1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the output of above layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def Recall_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def Precision_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def F1_score(y_true, y_pred):\n",
    "    precision = Precision_score(y_true, y_pred)\n",
    "    recall = Recall_score(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.01, decay=0.0005)\n",
    "\n",
    "## Callbacks\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5, verbose=1)\n",
    "\n",
    "concatenated = concatenate([out_num_cat,review_out_text],axis=-1)\n",
    "drop1 = Dropout(0.9)(concatenated)\n",
    "X1 = Dense(32, activation='relu',kernel_regularizer=regularizers.l2(),kernel_initializer='normal')(drop1)\n",
    "# drop2 = Dropout(0.2)(X1)\n",
    "# X2 = Dense(8, activation='relu',kernel_regularizer=regularizers.l2(),kernel_initializer='uniform')(drop2)\n",
    "final_out = Dense(1, activation='sigmoid')(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[num_cat_inputs,review_text_input], outputs=final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[F1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_train,review_text_seq_train,],\n",
    "          y=Y_train,\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          callbacks=[reduce_lr, earlystopper],\n",
    "          validation_data=([X_valid1,review_text_seq_valid1,],\n",
    "                           Y_valid1),\n",
    "          verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict([X_train,review_text_seq_train])\n",
    "valid1_classes = model.predict([X_valid1,review_text_seq_valid1])\n",
    "valid2_classes = model.predict([X_valid2,review_text_seq_valid2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = np.where(train_classes>0.5,1,0).flatten()\n",
    "valid1_classes = np.where(valid1_classes>0.5,1,0).flatten()\n",
    "valid2_classes = np.where(valid2_classes>0.5,1,0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train F1 Score :',round(f1_score(Y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(Y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(Y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_input= Input(shape=(REVIEW_TEXT_MAX_SEQUENCE_LENGTH,),name='review_text_input')\n",
    "if (review_word_not_in_glove_count<=1):\n",
    "    text_embed = Embedding(input_dim=NUM_WORDS_REVIEW_TEXT,output_dim=50,weights=[review_embedding_matrix],trainable=False)(review_text_input)\n",
    "    lstm1 = LSTM(32,dropout=0.4,return_sequences=True,kernel_initializer='normal',kernel_regularizer=regularizers.l2())(text_embed)\n",
    "else:\n",
    "    text_embed = Embedding(input_dim=NUM_WORDS_REVIEW_TEXT,output_dim=50,weights=[review_embedding_matrix],trainable=True)(review_text_input)\n",
    "    lstm1 = LSTM(32,dropout=0.4,return_sequences=True,kernel_initializer='normal',kernel_regularizer=regularizers.l2())(text_embed)\n",
    "review_out_text = Flatten()(lstm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.01, decay=0.0005)\n",
    "\n",
    "## Callbacks\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5, verbose=1)\n",
    "\n",
    "concatenated = concatenate([out_num_cat,review_out_text],axis=-1)\n",
    "drop1 = Dropout(0.9)(concatenated)\n",
    "X1 = Dense(32, activation='relu',kernel_regularizer=regularizers.l2(),kernel_initializer='normal')(drop1)\n",
    "# drop2 = Dropout(0.2)(X1)\n",
    "# X2 = Dense(8, activation='relu',kernel_regularizer=regularizers.l2(),kernel_initializer='uniform')(drop2)\n",
    "final_out = Dense(1, activation='sigmoid')(X1)\n",
    "model = Model(inputs=[num_cat_inputs,review_text_input], outputs=final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[F1_score])\n",
    "model.fit([X_train,review_text_seq_train,],\n",
    "          y=Y_train,\n",
    "          epochs=50,\n",
    "          batch_size=64,\n",
    "          callbacks=[reduce_lr, earlystopper],\n",
    "          validation_data=([X_valid1,review_text_seq_valid1,],\n",
    "                           Y_valid1),\n",
    "          verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict([X_train,review_text_seq_train])\n",
    "valid1_classes = model.predict([X_valid1,review_text_seq_valid1])\n",
    "valid2_classes = model.predict([X_valid2,review_text_seq_valid2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = np.where(train_classes>0.5,1,0).flatten()\n",
    "valid1_classes = np.where(valid1_classes>0.5,1,0).flatten()\n",
    "valid2_classes = np.where(valid2_classes>0.5,1,0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train F1 Score :',round(f1_score(Y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(Y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(Y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
