{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model Building on The Lion King (2019) Movie Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section\n",
    "1. I have extracted 2 sets of reviews from rotten tomatoes.\n",
    "    a. Train Set of 3000 reviews which can be split into train and valid1 sets and apply algorithms.\n",
    "    b. Valid2 set of 1100 reviews which can be used as a secondary validation set.\n",
    "2. I have tried different algorithms and SVM worked the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "random.seed(123)\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore','RuntimeWarning')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data1 is for train and valid1 split\n",
    "- valid2 is for using it as second unseen data\n",
    "- test is where we make predictions on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('alreviews_df_3000.csv')\n",
    "valid2 = pd.read_csv('alreviews_df_1100_validation.csv')\n",
    "test = pd.read_csv('test-1566381431512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 13)\n",
      "(1100, 12)\n",
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data1.shape)\n",
    "print(valid2.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2916\n",
      "1071\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data1.isna().sum().sum())\n",
    "print(valid2.isna().sum().sum())\n",
    "print(test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createDate</th>\n",
       "      <th>displayImageUrl</th>\n",
       "      <th>displayName</th>\n",
       "      <th>hasProfanity</th>\n",
       "      <th>hasSpoilers</th>\n",
       "      <th>isSuperReviewer</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>timeFromCreation</th>\n",
       "      <th>updateDate</th>\n",
       "      <th>primary_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2019-08-13T20:28:36.454Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>STAR_2</td>\n",
       "      <td>The small changes from the original to new were not good.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>2019-08-13T20:28:36.454Z</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   createDate displayImageUrl displayName  hasProfanity  \\\n",
       "500  2019-08-13T20:28:36.454Z  NaN             Lisa        False          \n",
       "\n",
       "     hasSpoilers  isSuperReviewer  isVerified  rating  \\\n",
       "500  False        False            True        STAR_2   \n",
       "\n",
       "                                                        review  score  \\\n",
       "500  The small changes from the original to new were not good.  2.0     \n",
       "\n",
       "    timeFromCreation                updateDate  primary_key  \n",
       "500  5d ago           2019-08-13T20:28:36.454Z  500          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>createDate</th>\n",
       "      <th>displayImageUrl</th>\n",
       "      <th>displayName</th>\n",
       "      <th>hasProfanity</th>\n",
       "      <th>hasSpoilers</th>\n",
       "      <th>isSuperReviewer</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "      <th>timeFromCreation</th>\n",
       "      <th>updateDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>2019-07-30T15:34:54.560Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lolmom</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>STAR_3</td>\n",
       "      <td>The humorous extras they added were fun. The imagery itself was not as impressive as the animated version, just because it's all been done before. The vocals did not live up to the original either in some cases.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Jul 30, 2019</td>\n",
       "      <td>2019-07-30T15:34:54.560Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   createDate displayImageUrl displayName  hasProfanity  \\\n",
       "830  2019-07-30T15:34:54.560Z  NaN             Lolmom      False          \n",
       "\n",
       "     hasSpoilers  isSuperReviewer  isVerified  rating  \\\n",
       "830  False        False            False       STAR_3   \n",
       "\n",
       "                                                                                                                                                                                                                  review  \\\n",
       "830  The humorous extras they added were fun. The imagery itself was not as impressive as the animated version, just because it's all been done before. The vocals did not live up to the original either in some cases.   \n",
       "\n",
       "     score timeFromCreation                updateDate  \n",
       "830  3.0    Jul 30, 2019     2019-07-30T15:34:54.560Z  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid2.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>93442</td>\n",
       "      <td>IT WAS AMAZING THE MOVIE KICKED ASS!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ReviewID                                  Review\n",
       "566  93442     IT WAS AMAZING THE MOVIE KICKED ASS!!!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['createDate','displayImageUrl','displayName','hasProfanity','hasSpoilers','isSuperReviewer','isVerified','rating','timeFromCreation','updateDate','primary_key'],axis=1,inplace=True)\n",
    "valid2.drop(['createDate','displayImageUrl','displayName','hasProfanity','hasSpoilers','isSuperReviewer','isVerified','rating','timeFromCreation','updateDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Really enjoyed it. The songs were amazing and the visuals were spectacular. I see what people are saying about it’s like if emotion and dullness, but I personally liked it and if you ever liked the original than you’ll like this. The acting and singing was amazing!</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Realky enjoyable. We've seen the original animated movie, the stage play and now the live action movie and it's a worthy extension of the story.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beautiful! Loved it!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolutely loved the movie, it had my emotions all over the place. Thank you for an awesome experience.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tha movie was phenomenal</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                      review  \\\n",
       "0  Really enjoyed it. The songs were amazing and the visuals were spectacular. I see what people are saying about it’s like if emotion and dullness, but I personally liked it and if you ever liked the original than you’ll like this. The acting and singing was amazing!   \n",
       "1  Realky enjoyable. We've seen the original animated movie, the stage play and now the live action movie and it's a worthy extension of the story.                                                                                                                            \n",
       "2  Beautiful! Loved it!                                                                                                                                                                                                                                                        \n",
       "3  Absolutely loved the movie, it had my emotions all over the place. Thank you for an awesome experience.                                                                                                                                                                     \n",
       "4  Tha movie was phenomenal                                                                                                                                                                                                                                                    \n",
       "\n",
       "   score  \n",
       "0  4.0    \n",
       "1  5.0    \n",
       "2  5.0    \n",
       "3  5.0    \n",
       "4  5.0    "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought it really good glad they didn't make scar a queer</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can only say this movie is good in animate this movie feels realistic but its not original The Lion King movie so I enjoy it as much as Im very disappointed in it. Id give it 4 half star for its movie but 1 star for the title because its not exact same as the 1994 The Lion King movie.</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great songs-----forgot it was animated</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The kids and adults love the movie!</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely loved it!! Heyyy what better way to release a New movie... Its Loe Season</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                            review  \\\n",
       "0  I thought it really good glad they didn't make scar a queer                                                                                                                                                                                                                                       \n",
       "1  I can only say this movie is good in animate this movie feels realistic but its not original The Lion King movie so I enjoy it as much as Im very disappointed in it. Id give it 4 half star for its movie but 1 star for the title because its not exact same as the 1994 The Lion King movie.   \n",
       "2  Great songs-----forgot it was animated                                                                                                                                                                                                                                                            \n",
       "3  The kids and adults love the movie!                                                                                                                                                                                                                                                               \n",
       "4  Absolutely loved it!! Heyyy what better way to release a New movie... Its Loe Season                                                                                                                                                                                                              \n",
       "\n",
       "   score  \n",
       "0  4.0    \n",
       "1  3.0    \n",
       "2  4.5    \n",
       "3  5.0    \n",
       "4  5.0    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a target 'sentiment' from score\n",
    "### Positive : 0\n",
    "### Negative : 1\n",
    "\n",
    "- Since the metric is f1 score for negative reviews. We are assigning 1 to negative reviews and 0 to positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['sentiment'] = np.where((data1['score']>3.0),0,1)\n",
    "valid2['sentiment'] = np.where((valid2['score']>3.0),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.723333\n",
       "1    0.276667\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.75\n",
       "1    0.25\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid2.sentiment.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    #re.compile(regex).search(subject) is equivalent to re.search(regex, subject).\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = re.sub(\"’\", \"'\", text)\n",
    "    expanded_text = contractions_pattern.sub(expand_match, expanded_text)\n",
    "\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the Reviews\n",
    "def clean_doc(doc):\n",
    "    # Removing contractions\n",
    "    doc = expand_contractions(doc)\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split(' ')\n",
    "    \n",
    "    # Converting into lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove special characters from each token\n",
    "    tokens = [re.sub(r\"[^a-zA-Z#\\s]\",'',i) for i in tokens]\n",
    "    tokens = [re.sub(r\"[\\r\\n]\",'',i) for i in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['modified_review'] = data1.review.apply(lambda x: ' '.join(clean_doc(x)))\n",
    "valid2['modified_review'] = valid2.review.apply(lambda x: ' '.join(clean_doc(x)))\n",
    "test['modified_review'] = test.Review.apply(lambda x: ' '.join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dependent and independent variables.\n",
    "X = data1['modified_review']\n",
    "y = data1['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid1, y_train, y_valid1 = train_test_split(X,y,test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid2 = valid2['modified_review']\n",
    "y_valid2 = valid2['sentiment']\n",
    "X_test = test['modified_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100,)\n",
      "(2100,)\n",
      "(900,)\n",
      "(900,)\n",
      "(1100,)\n",
      "(1100,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid1.shape)\n",
    "print(y_valid1.shape)\n",
    "print(X_valid2.shape)\n",
    "print(y_valid2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the reviews using TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90,max_features=1000,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid1_tfidf = tfidf_vectorizer.transform(X_valid1)\n",
    "X_valid2_tfidf = tfidf_vectorizer.transform(X_valid2)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the reviews using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english',lowercase=True, strip_accents='unicode',decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cv = count_vectorizer.fit_transform(X_train)\n",
    "X_valid1_cv = count_vectorizer.transform(X_valid1)\n",
    "X_valid2_cv = count_vectorizer.transform(X_valid2)\n",
    "\n",
    "X_test_cv = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.77\n",
      "Valid1 F1 Score : 0.713\n",
      "Valid2 F1 Score : 0.701\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty='l2',class_weight='balanced',C=0.5)\n",
    "lr_clf = logreg.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = lr_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = lr_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = lr_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "- Balanced class weight parameter is found to increase the F1 score\n",
    "- As C increases the model overfits.\n",
    "- As C decrease below 0.5 the model underfits.\n",
    "- L2 penalty is found to be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.827\n",
      "Valid1 F1 Score : 0.682\n",
      "Valid2 F1 Score : 0.714\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty='l1',class_weight='balanced',C=0.9)\n",
    "lr_clf = logreg.fit(X_train_cv,y_train)\n",
    "\n",
    "train_pred = lr_clf.predict(X_train_cv)\n",
    "\n",
    "valid1_pred = lr_clf.predict(X_valid1_cv)\n",
    "\n",
    "valid2_pred = lr_clf.predict(X_valid2_cv)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "- Balanced class weight parameter is found to increase the F1 score\n",
    "- As C increases the model overfits.\n",
    "- As C decrease below 0.9 the model underfits.\n",
    "- L1 penalty is found to be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.798\n",
      "Valid1 F1 Score : 0.581\n",
      "Valid2 F1 Score : 0.594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB(alpha=0.1)\n",
    "NB_clf = classifier.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = NB_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = NB_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = NB_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Count Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.859\n",
      "Valid1 F1 Score : 0.647\n",
      "Valid2 F1 Score : 0.664\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB(alpha=0.5)\n",
    "NB_clf = classifier.fit(X_train_cv,y_train)\n",
    "\n",
    "train_pred = NB_clf.predict(X_train_cv)\n",
    "\n",
    "valid1_pred = NB_clf.predict(X_valid1_cv)\n",
    "\n",
    "valid2_pred = NB_clf.predict(X_valid2_cv)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.834\n",
      "Valid1 F1 Score : 0.596\n",
      "Valid2 F1 Score : 0.635\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.SVC(kernel='linear')\n",
    "svm_clf = svm_classifier.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.831\n",
      "Valid1 F1 Score : 0.698\n",
      "Valid2 F1 Score : 0.685\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = svm.SVC(kernel=\"linear\", class_weight=\"balanced\")\n",
    "svm_clf = svm_classifier.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = svm_clf.predict(X_test_tfidf)\n",
    "\n",
    "# pd.Series(test_pred).value_counts(normalize=True)\n",
    "\n",
    "# submission = pd.read_csv('samplesubmission.csv')\n",
    "\n",
    "# submission.sentiment = test_pred.astype('int64')\n",
    "\n",
    "# submission.sentiment.dtype\n",
    "\n",
    "# submission.to_csv('submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adaboost_model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=600,learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.99\n",
      "Valid1 F1 Score : 0.565\n",
      "Valid2 F1 Score : 0.587\n"
     ]
    }
   ],
   "source": [
    "adb_clf = Adaboost_model.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = adb_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = adb_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = adb_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Grid Search with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators':[100,150,200],\n",
    "             'learning_rate':[0.1,0.5,0.9]}\n",
    "\n",
    "Adaboost_model_grid = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier(max_depth=2)),param_grid,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 150}\n",
      "Train F1 Score : 0.787\n",
      "Valid1 F1 Score : 0.499\n",
      "Valid2 F1 Score : 0.527\n"
     ]
    }
   ],
   "source": [
    "adb_clf = Adaboost_model_grid.fit(X_train_tfidf,y_train)\n",
    "\n",
    "print(Adaboost_model_grid.best_params_)\n",
    "\n",
    "train_pred = adb_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = adb_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = adb_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Grid Search with CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'n_estimators': 200}\n",
      "Train F1 Score : 0.781\n",
      "Valid1 F1 Score : 0.54\n",
      "Valid2 F1 Score : 0.549\n"
     ]
    }
   ],
   "source": [
    "adb_clf = Adaboost_model_grid.fit(X_train_cv,y_train)\n",
    "\n",
    "print(Adaboost_model_grid.best_params_)\n",
    "\n",
    "train_pred = adb_clf.predict(X_train_cv)\n",
    "\n",
    "valid1_pred = adb_clf.predict(X_valid1_cv)\n",
    "\n",
    "valid2_pred = adb_clf.predict(X_valid2_cv)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.782\n",
      "Valid1 F1 Score : 0.526\n",
      "Valid2 F1 Score : 0.552\n"
     ]
    }
   ],
   "source": [
    "GBM_model = GradientBoostingClassifier(n_estimators=50,learning_rate=0.3,subsample=0.8)\n",
    "gbm_clf = GBM_model.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = gbm_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = gbm_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = gbm_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting with CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.754\n",
      "Valid1 F1 Score : 0.531\n",
      "Valid2 F1 Score : 0.534\n"
     ]
    }
   ],
   "source": [
    "GBM_model = GradientBoostingClassifier(n_estimators=50,learning_rate=0.3,subsample=0.8)\n",
    "gbm_clf = GBM_model.fit(X_train_cv,y_train)\n",
    "\n",
    "train_pred = gbm_clf.predict(X_train_cv)\n",
    "\n",
    "valid1_pred = gbm_clf.predict(X_valid1_cv)\n",
    "\n",
    "valid2_pred = gbm_clf.predict(X_valid2_cv)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.847\n",
      "Valid1 F1 Score : 0.682\n",
      "Valid2 F1 Score : 0.695\n"
     ]
    }
   ],
   "source": [
    "svm_classifier_linear = svm.SVC(kernel=\"linear\", class_weight=\"balanced\",C=0.09)\n",
    "\n",
    "svm_clf = svm_classifier_linear.fit(X_train_cv,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_cv)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_cv)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_cv)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred = svm_clf.predict(X_test_cv)\n",
    "\n",
    "# pd.Series(test_pred).value_counts(normalize=True)\n",
    "\n",
    "# submission = pd.read_csv('samplesubmission.csv')\n",
    "\n",
    "# submission.sentiment = test_pred.astype('int64')\n",
    "\n",
    "# submission.sentiment.dtype\n",
    "\n",
    "# submission.to_csv('submission2.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.831\n",
      "Valid1 F1 Score : 0.698\n",
      "Valid2 F1 Score : 0.685\n"
     ]
    }
   ],
   "source": [
    "svm_classifier_linear = svm.SVC(kernel=\"linear\", class_weight=\"balanced\",C=1)\n",
    "svm_clf = svm_classifier_linear.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search on Gradient Boosting with CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "              'learning_rate':[0.1,0.5],\n",
    "              'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n",
    "\n",
    "GBM_model_grid = GridSearchCV(GradientBoostingClassifier(n_estimators=200,),param_grid,n_jobs=-1,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.763\n",
      "Valid1 F1 Score : 0.563\n",
      "Valid2 F1 Score : 0.559\n"
     ]
    }
   ],
   "source": [
    "gbm_clf = GBM_model_grid.fit(X_train_cv,y_train)\n",
    "\n",
    "train_pred = gbm_clf.predict(X_train_cv)\n",
    "\n",
    "valid1_pred = gbm_clf.predict(X_valid1_cv)\n",
    "\n",
    "valid2_pred = gbm_clf.predict(X_valid2_cv)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'subsample': 0.8}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBM_model_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.87\n",
      "Valid1 F1 Score : 0.58\n",
      "Valid2 F1 Score : 0.63\n"
     ]
    }
   ],
   "source": [
    "GBM_model = GradientBoostingClassifier(random_state=123,n_estimators=300,learning_rate=0.5,min_samples_leaf=10,max_features=25,max_depth=2)\n",
    "\n",
    "gbm_clf = GBM_model.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = gbm_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = gbm_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = gbm_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVec using Ngrams(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizern2 = CountVectorizer(stop_words='english',lowercase=True, strip_accents='unicode',decode_error='ignore',ngram_range=(1,2))\n",
    "count_vectorizern3 = CountVectorizer(stop_words='english',lowercase=True, strip_accents='unicode',decode_error='ignore',ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvn2 = count_vectorizern2.fit_transform(X_train)\n",
    "X_valid1_cvn2 = count_vectorizern2.transform(X_valid1)\n",
    "X_valid2_cvn2 = count_vectorizern2.transform(X_valid2)\n",
    "\n",
    "X_test_cvn2 = count_vectorizern2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cvn3 = count_vectorizern3.fit_transform(X_train)\n",
    "X_valid1_cvn3 = count_vectorizern3.transform(X_valid1)\n",
    "X_valid2_cvn3 = count_vectorizern3.transform(X_valid2)\n",
    "\n",
    "X_test_cvn3 = count_vectorizern3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.891\n",
      "Valid1 F1 Score : 0.673\n",
      "Valid2 F1 Score : 0.693\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty='l2',class_weight='balanced',C=0.1)\n",
    "lr_clf = logreg.fit(X_train_cvn2,y_train)\n",
    "\n",
    "train_pred = lr_clf.predict(X_train_cvn2)\n",
    "\n",
    "valid1_pred = lr_clf.predict(X_valid1_cvn2)\n",
    "\n",
    "valid2_pred = lr_clf.predict(X_valid2_cvn2)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.918\n",
      "Valid1 F1 Score : 0.673\n",
      "Valid2 F1 Score : 0.683\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty='l2',class_weight='balanced',C=0.08)\n",
    "lr_clf = logreg.fit(X_train_cvn3,y_train)\n",
    "\n",
    "train_pred = lr_clf.predict(X_train_cvn3)\n",
    "\n",
    "valid1_pred = lr_clf.predict(X_valid1_cvn3)\n",
    "\n",
    "valid2_pred = lr_clf.predict(X_valid2_cvn3)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.921\n",
      "Valid1 F1 Score : 0.652\n",
      "Valid2 F1 Score : 0.676\n"
     ]
    }
   ],
   "source": [
    "svm_classifier_linear = svm.SVC(kernel=\"linear\", class_weight=\"balanced\",C=0.05)\n",
    "\n",
    "svm_clf = svm_classifier_linear.fit(X_train_cvn2,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_cvn2)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_cvn2)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_cvn2)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.961\n",
      "Valid1 F1 Score : 0.63\n",
      "Valid2 F1 Score : 0.656\n"
     ]
    }
   ],
   "source": [
    "svm_classifier_linear = svm.SVC(kernel=\"linear\", class_weight=\"balanced\",C=0.05)\n",
    "\n",
    "svm_clf = svm_classifier_linear.fit(X_train_cvn3,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_cvn3)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_cvn3)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_cvn3)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
