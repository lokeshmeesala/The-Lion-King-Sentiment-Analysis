{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement\n",
    "\n",
    "### In this Section\n",
    "- The idea in this code is to have equal ratio of positive and negative reviews in our dataset to address any minor class    imbalance.\n",
    "- The original data has 72% of positive reviews and 28% of negative reviews.\n",
    "- I have collected 8040 reviews and created a train set of 3000 reviews having 54% of positive reviews and 46% of negative reviews.\n",
    "- I have also created a validation set of 1200 reviews to perform validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "random.seed(123)\n",
    "import datetime as dt\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore','RuntimeWarning')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    #re.compile(regex).search(subject) is equivalent to re.search(regex, subject).\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = re.sub(\"â€™\", \"'\", text)\n",
    "    expanded_text = contractions_pattern.sub(expand_match, expanded_text)\n",
    "\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the Reviews\n",
    "def clean_doc(doc):\n",
    "    # Removing contractions\n",
    "    doc = expand_contractions(doc)\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split(' ')\n",
    "    \n",
    "    # Converting into lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove special characters from each token\n",
    "    tokens = [re.sub(r\"[^a-zA-Z#\\s]\",'',i) for i in tokens]\n",
    "    tokens = [re.sub(r\"[\\r\\n]\",'',i) for i in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with 8040 reviews\n",
    "data8000 = pd.read_csv('alreviews_df_8000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8000.drop(['createDate', 'displayImageUrl', 'displayName', 'hasProfanity',\n",
    "       'hasSpoilers', 'isSuperReviewer', 'isVerified', 'rating','timeFromCreation', 'updateDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8000['sentiment'] = np.where((data8000['score']>3.0),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8000['modified_review'] = data8000.review.apply(lambda x: ' '.join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if any empty strings are created after pre processing\n",
    "nulls = data8000[data8000.modified_review.apply(lambda x: len(x)) == 0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8000.drop(nulls,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8000['primary_key']= list(range(len(data8000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating positive and negative reviews\n",
    "pos_reviews = data8000[data8000.sentiment == 0]\n",
    "neg_reviews = data8000[data8000.sentiment == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating positive and negative reviews for training set\n",
    "pos_reviews_train = pos_reviews[pos_reviews.sentiment==0].sample(n=1600)\n",
    "neg_reviews_train = neg_reviews[neg_reviews.sentiment==1].sample(n=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are seperating the reviews that are not present in reviews_train\n",
    "pos_reviews_valid2 = pd.concat([pos_reviews,pos_reviews_train]).drop_duplicates(keep=False,subset='primary_key')\n",
    "neg_reviews_valid2 = pd.concat([neg_reviews,neg_reviews_train]).drop_duplicates(keep=False,subset='primary_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating positive and negative reviews for train set and joining them to create train set\n",
    "data5050 = pd.concat([pos_reviews_train,neg_reviews_train],ignore_index=True,axis=0)\n",
    "valid2_full = pd.concat([pos_reviews_valid2,neg_reviews_valid2],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1600\n",
      "1    1400\n",
      "Name: sentiment, dtype: int64\n",
      "0    4248\n",
      "1    789 \n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data5050.sentiment.value_counts())\n",
    "print(valid2_full.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating positive and negative reviews for validation set and joining them to create validation set\n",
    "pos_reviews_valid = valid2_full[valid2_full.sentiment == 0].sample(n=800)\n",
    "neg_reviews_valid = valid2_full[valid2_full.sentiment == 1].sample(n=400)\n",
    "valid2 = pd.concat([pos_reviews_valid,neg_reviews_valid],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    800\n",
      "1    400\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(valid2.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Making sure that there are no duplicate reviews in train and valid2 set and between them.\n",
    "print(data5050.duplicated(subset='primary_key').sum())\n",
    "print(valid2.duplicated(subset='primary_key').sum())\n",
    "print(pd.concat([valid2,data5050],ignore_index=True,axis=0).duplicated(subset='primary_key').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1600\n",
      "1    1400\n",
      "Name: sentiment, dtype: int64\n",
      "0    800\n",
      "1    400\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data5050.sentiment.value_counts())\n",
    "print(valid2.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffleing the reviews\n",
    "data5050 = data5050.sample(n=3000)\n",
    "valid2 = valid2.sample(n=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data5050.to_csv('final_allreviews_5050.csv',index=False)\n",
    "#valid2.to_csv('final_valid2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1=pd.read_csv('final_allreviews_5050.csv')\n",
    "# valid2=pd.read_csv('final_valid2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test-1566381431512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['modified_review'] = test.Review.apply(lambda x: ' '.join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Review</th>\n",
       "      <th>modified_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>93360</td>\n",
       "      <td>Loved the music. Still a Great story. Animation was fantastic.</td>\n",
       "      <td>loved music still great story animation fantastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ReviewID                                                          Review  \\\n",
       "484  93360     Loved the music. Still a Great story. Animation was fantastic.   \n",
       "\n",
       "                                       modified_review  \n",
       "484  loved music still great story animation fantastic  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data5050.modified_review\n",
    "y = data5050.sentiment\n",
    "\n",
    "X_train, X_valid1, y_train, y_valid1 = train_test_split(X,y,test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid2 = valid2.modified_review\n",
    "y_valid2 = valid2.sentiment\n",
    "X_test = test.Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_words = 10000\n",
    "seq_len = 50\n",
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_num_words) #Tokenizer is used to tokenize text\n",
    "tokenizer.fit_on_texts(X_train) #Fit this to our corpus\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) #'text to sequences converts the text to a list of indices\n",
    "X_train = pad_sequences(X_train, maxlen=50) #pad_sequences makes every sequence a fixed size list by padding with 0s \n",
    "X_valid1 = tokenizer.texts_to_sequences(X_valid1) \n",
    "X_valid1 = pad_sequences(X_valid1, maxlen=50)\n",
    "X_valid2 = tokenizer.texts_to_sequences(X_valid2) \n",
    "X_valid2 = pad_sequences(X_valid2, maxlen=50)\n",
    "X_test = tokenizer.texts_to_sequences(X_test) \n",
    "X_test = pad_sequences(X_test, maxlen=50)\n",
    "\n",
    "X_train.shape, X_valid1.shape, X_valid2.shape, X_test.shape # Check the dimensions of x_train and x_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def F1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=\"weights/cnn_weights-improvement-{epoch:02d}-{val_F1_score:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, \n",
    "#                              monitor='val_F1_score', \n",
    "#                              verbose=1, \n",
    "#                              mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_num_words,input_length = seq_len,output_dim = embedding_size))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[F1_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_data=(X_valid1, y_valid1),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Past Results:\n",
    "    Train F1 Score : 1.0\n",
    "    Valid1 F1 Score : 0.79\n",
    "    Valid2 F1 Score : 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes_cnn = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('samplesubmission.csv')\n",
    "\n",
    "# submission.sentiment = test_classes.astype('int64')\n",
    "\n",
    "# submission.sentiment.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv('submission8.csv',index=False)\n",
    "# Train F1 Score : 1.0\n",
    "# Valid1 F1 Score : 0.79\n",
    "# Valid2 F1 Score : 0.74\n",
    "#test_score is 69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, LSTM\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU=Sequential()\n",
    "model_GRU.add(Embedding(max_num_words,100,mask_zero=True))\n",
    "model_GRU.add(GRU(32,dropout=0.9,return_sequences=True))\n",
    "model_GRU.add(GRU(16,dropout=0.9,return_sequences=False))\n",
    "model_GRU.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=[F1_score])\n",
    "model_GRU.fit(X_train, y_train,batch_size=16,\n",
    "                            epochs=12,\n",
    "                            validation_data=(X_valid1, y_valid1),\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model_GRU.predict_classes(X_train)\n",
    "valid1_classes = model_GRU.predict_classes(X_valid1)\n",
    "valid2_classes = model_GRU.predict_classes(X_valid2)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Past Results:\n",
    "    Train F1 Score : 0.93\n",
    "    Valid1 F1 Score : 0.79\n",
    "    Valid2 F1 Score : 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes_gru = model_GRU.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('test-1566381431512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['sentiment_gru'] = test_classes_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['sentiment_cnn'] = test_classes_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM=Sequential()\n",
    "model_LSTM.add(Embedding(max_num_words,100,mask_zero=True))\n",
    "model_LSTM.add(LSTM(64,dropout=0.9,return_sequences=True))\n",
    "model_LSTM.add(LSTM(64,dropout=0.9,return_sequences=False))\n",
    "model_LSTM.add(Dense(1,activation='sigmoid'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_LSTM.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.01, decay=0.001),metrics=[F1_score])\n",
    "model_LSTM.fit(X_train, y_train,batch_size=32,\n",
    "                            epochs=10,\n",
    "                            validation_data=(X_valid1, y_valid1),\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model_LSTM.predict_classes(X_train)\n",
    "valid1_classes = model_LSTM.predict_classes(X_valid1)\n",
    "valid2_classes = model_LSTM.predict_classes(X_valid2)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Past Results:\n",
    "    Train F1 Score : 0.95\n",
    "    Valid1 F1 Score : 0.81\n",
    "    Valid2 F1 Score : 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes_lstm = model_LSTM.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['sentiment_lstm'] = test_classes_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.sentiment_cnn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.sentiment_gru.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.sentiment_lstm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into the reviews where each model had different predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1[~(((test1.sentiment_cnn ==1)&(test1.sentiment_gru ==1)&(test1.sentiment_lstm ==1))|\n",
    "     ((test1.sentiment_cnn ==0)&(test1.sentiment_gru ==0)&(test1.sentiment_lstm ==0)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.drop(['ReviewID','Review'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['mode1']= test1.mode(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('samplesubmission.csv')\n",
    "\n",
    "# submission.sentiment = test1.mode1\n",
    "\n",
    "# submission.sentiment.dtype\n",
    "\n",
    "# submission.sentiment.value_counts()\n",
    "\n",
    "# submission.to_csv('submission9.csv',index=False)\n",
    "\n",
    "# Test score = 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are trying ML models on unprocessed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90,max_features=1000,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data5050.review\n",
    "y = data5050.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid1, y_train, y_valid1 = train_test_split(X,y,test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid2 = valid2['review']\n",
    "y_valid2 = valid2['sentiment']\n",
    "X_test = test['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(2400,)\n",
      "(600,)\n",
      "(600,)\n",
      "(1200,)\n",
      "(1200,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid1.shape)\n",
    "print(y_valid1.shape)\n",
    "print(X_valid2.shape)\n",
    "print(y_valid2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_valid1_tfidf = tfidf_vectorizer.transform(X_valid1)\n",
    "X_valid2_tfidf = tfidf_vectorizer.transform(X_valid2)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.882\n",
      "Valid1 F1 Score : 0.803\n",
      "Valid2 F1 Score : 0.775\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty='l2',C=1.25)\n",
    "lr_clf = logreg.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = lr_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = lr_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = lr_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Past Results:\n",
    "    Train F1 Score : 0.882\n",
    "    Valid1 F1 Score : 0.803\n",
    "    Valid2 F1 Score : 0.775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB(alpha=0.1)\n",
    "NB_clf = classifier.fit(X_train_tfidf,y_train)\n",
    "\n",
    "train_pred = NB_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = NB_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = NB_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_grid = {'C':[4,6,8],'gamma': [2,2.5,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_model_grid = GridSearchCV(SVC(kernel='rbf'),param_grid,n_jobs=-1,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel=\"rbf\",C=25,gamma=2)\n",
    "svm_clf = svm_classifier.fit(X_train_tfidf,y_train)\n",
    "#svm_clf = svm_model_grid.fit(X_train_tfidf,y_train)\n",
    "\n",
    "\n",
    "train_pred = svm_clf.predict(X_train_tfidf)\n",
    "\n",
    "valid1_pred = svm_clf.predict(X_valid1_tfidf)\n",
    "\n",
    "valid2_pred = svm_clf.predict(X_valid2_tfidf)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),3))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),3))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM REPORT\n",
    "<br><br>\n",
    "{'C': 10, 'gamma': 1}\n",
    "Train F1 Score : 0.995\n",
    "Valid1 F1 Score : 0.803\n",
    "Valid2 F1 Score : 0.813\n",
    "<br><br>\n",
    "{'C': 6, 'gamma': 2}\n",
    "Train F1 Score : 0.995\n",
    "Valid1 F1 Score : 0.826\n",
    "Valid2 F1 Score : 0.817\n",
    "<br><br>\n",
    "{'C': 4, 'gamma': 2}\n",
    "Train F1 Score : 0.995\n",
    "Valid1 F1 Score : 0.824\n",
    "Valid2 F1 Score : 0.815\n",
    "<br><br>\n",
    "{'C': 10, 'gamma': 2}\n",
    "Train F1 Score : 0.995\n",
    "Valid1 F1 Score : 0.821\n",
    "Valid2 F1 Score : 0.825\n",
    "<br><br>\n",
    "{'C': 6, 'gamma': 1.8}\n",
    "Train F1 Score : 0.995\n",
    "Valid1 F1 Score : 0.827\n",
    "Valid2 F1 Score : 0.828\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
