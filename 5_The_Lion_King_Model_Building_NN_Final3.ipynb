{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Building on The Lion King (2019) Movie Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section\n",
    "1. I have applied various neural network algorithmns and 1D CNN was the best.\n",
    "2. I have built an SVM classifier with the output of the hidden layer.\n",
    "3. I have built a model using Pretrained Embeddings from GLOVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lokesh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore','RuntimeWarning')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import GlobalMaxPooling1D, AveragePooling1D,Conv1D,Dropout,Embedding, MaxPooling1D,GRU,Bidirectional,LSTM,SpatialDropout1D,GlobalMaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('alreviews_df_3000.csv')\n",
    "valid2 = pd.read_csv('alreviews_df_1100_validation.csv')\n",
    "test = pd.read_csv('test-1566381431512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['createDate','displayImageUrl','displayName','hasProfanity','hasSpoilers','isSuperReviewer','isVerified','rating','timeFromCreation','updateDate','primary_key'],axis=1,inplace=True)\n",
    "valid2.drop(['createDate','displayImageUrl','displayName','hasProfanity','hasSpoilers','isSuperReviewer','isVerified','rating','timeFromCreation','updateDate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['sentiment'] = np.where((data1['score']>3.0),0,1)\n",
    "valid2['sentiment'] = np.where((valid2['score']>3.0),0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": 'is not', \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"'cause\": 'because', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he'll've\": 'he he will have', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'd've\": 'I would have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'd've\": 'i would have', \"i'll\": 'i will', \"i'll've\": 'i will have', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'd've\": 'it would have', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it's\": 'it is', \"let's\": 'let us', \"ma'am\": 'madam', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"o'clock\": 'of the clock', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shan't've\": 'shall not have', \"she'd\": 'she would', \"she'd've\": 'she would have', \"she'll\": 'she will', \"she'll've\": 'she will have', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so as', \"that'd\": 'that would', \"that'd've\": 'that would have', \"that's\": 'that is', \"there'd\": 'there would', \"there'd've\": 'there would have', \"there's\": 'there is', \"they'd\": 'they would', \"they'd've\": 'they would have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', \"to've\": 'to have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what'll\": 'what will', \"what'll've\": 'what will have', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"when's\": 'when is', \"when've\": 'when have', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"why've\": 'why have', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you will have', \"you're\": 'you are', \"you've\": 'you have'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    #re.compile(regex).search(subject) is equivalent to re.search(regex, subject).\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "            if contraction_mapping.get(match)\\\n",
    "            else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = re.sub(\"â€™\", \"'\", text)\n",
    "    expanded_text = contractions_pattern.sub(expand_match, expanded_text)\n",
    "\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Preprocess the Reviews\n",
    "def clean_doc(doc):\n",
    "    # Removing contractions\n",
    "    doc = expand_contractions(doc)\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split(' ')\n",
    "    \n",
    "    # Converting into lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove special characters from each token\n",
    "    tokens = [re.sub(r\"[^a-zA-Z#\\s]\",'',i) for i in tokens]\n",
    "    tokens = [re.sub(r\"[\\r\\n]\",'',i) for i in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(w) for w in tokens]\n",
    "    \n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.review = data1.review.apply(lambda x: ' '.join(clean_doc(x)))\n",
    "valid2.review = valid2.review.apply(lambda x: ' '.join(clean_doc(x)))\n",
    "test.Review = test.Review.apply(lambda x: ' '.join(clean_doc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data1.review\n",
    "y = data1.sentiment\n",
    "\n",
    "X_train, X_valid1, y_train, y_valid1 = train_test_split(X,y,test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid2 = valid2.review\n",
    "y_valid2 = valid2.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_words = 10000\n",
    "seq_len = 50\n",
    "embedding_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2100, 50), (900, 50), (1100, 50), (1200, 50))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_num_words) #Tokenizer is used to tokenize text\n",
    "tokenizer.fit_on_texts(X_train) #Fit this to our corpus\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train) #'text to sequences converts the text to a list of indices\n",
    "X_train = pad_sequences(X_train, maxlen=50) #pad_sequences makes every sequence a fixed size list by padding with 0s \n",
    "X_valid1 = tokenizer.texts_to_sequences(X_valid1) \n",
    "X_valid1 = pad_sequences(X_valid1, maxlen=50)\n",
    "X_valid2 = tokenizer.texts_to_sequences(X_valid2) \n",
    "X_valid2 = pad_sequences(X_valid2, maxlen=50)\n",
    "X_test = tokenizer.texts_to_sequences(X_test) \n",
    "X_test = pad_sequences(X_test, maxlen=50)\n",
    "\n",
    "X_train.shape, X_valid1.shape, X_valid2.shape, X_test.shape # Check the dimensions of x_train and x_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def Recall_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def Precision_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def F1_score(y_true, y_pred):\n",
    "    precision = Precision_score(y_true, y_pred)\n",
    "    recall = Recall_score(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                250050    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 1,251,351\n",
      "Trainable params: 1,251,351\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_num_words,input_length = seq_len,output_dim = embedding_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.9))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[F1_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/20\n",
      " - 1s - loss: 0.5765 - F1_score: 0.0554 - val_loss: 0.5603 - val_F1_score: 0.0403\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.5568 - F1_score: 0.1169 - val_loss: 0.5474 - val_F1_score: 0.1263\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.5426 - F1_score: 0.2536 - val_loss: 0.5312 - val_F1_score: 0.2274\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.5040 - F1_score: 0.3920 - val_loss: 0.4964 - val_F1_score: 0.3270\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.4336 - F1_score: 0.5499 - val_loss: 0.4515 - val_F1_score: 0.3650\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.3555 - F1_score: 0.6934 - val_loss: 0.4429 - val_F1_score: 0.4038\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.3099 - F1_score: 0.7333 - val_loss: 0.4333 - val_F1_score: 0.4602\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.2655 - F1_score: 0.7882 - val_loss: 0.4888 - val_F1_score: 0.4002\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.2632 - F1_score: 0.7858 - val_loss: 0.4383 - val_F1_score: 0.5286\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.2345 - F1_score: 0.8295 - val_loss: 0.4739 - val_F1_score: 0.4901\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.2132 - F1_score: 0.8412 - val_loss: 0.4583 - val_F1_score: 0.5392\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.1962 - F1_score: 0.8512 - val_loss: 0.4975 - val_F1_score: 0.4918\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.1736 - F1_score: 0.8652 - val_loss: 0.4829 - val_F1_score: 0.5247\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.1763 - F1_score: 0.8705 - val_loss: 0.4702 - val_F1_score: 0.5640\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.1451 - F1_score: 0.8880 - val_loss: 0.4878 - val_F1_score: 0.5552\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.1449 - F1_score: 0.9036 - val_loss: 0.5092 - val_F1_score: 0.5562\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.1326 - F1_score: 0.9023 - val_loss: 0.5527 - val_F1_score: 0.5035\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.1257 - F1_score: 0.9040 - val_loss: 0.5532 - val_F1_score: 0.5321\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.1263 - F1_score: 0.9084 - val_loss: 0.5082 - val_F1_score: 0.6098\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.1254 - F1_score: 0.9037 - val_loss: 0.5321 - val_F1_score: 0.5580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4973bf208>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_valid1, y_valid1), epochs=20, batch_size=64, verbose=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.97\n",
      "Valid1 F1 Score : 0.56\n",
      "Valid2 F1 Score : 0.55\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 50, 32)            9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 250)               200250    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,210,133\n",
      "Trainable params: 1,210,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_num_words,input_length = seq_len,output_dim = embedding_size))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[F1_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      " - 62s - loss: 0.6100 - F1_score: 0.0254 - val_loss: 0.5927 - val_F1_score: 0.0000e+00\n",
      "Epoch 2/5\n",
      " - 49s - loss: 0.5610 - F1_score: 0.0029 - val_loss: 0.5540 - val_F1_score: 0.0545\n",
      "Epoch 3/5\n",
      " - 49s - loss: 0.5329 - F1_score: 0.2215 - val_loss: 0.5250 - val_F1_score: 0.3356\n",
      "Epoch 4/5\n",
      " - 49s - loss: 0.4578 - F1_score: 0.3987 - val_loss: 0.4475 - val_F1_score: 0.3455\n",
      "Epoch 5/5\n",
      " - 50s - loss: 0.3280 - F1_score: 0.7043 - val_loss: 0.4161 - val_F1_score: 0.5377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a498c20e10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_valid1, y_valid1), epochs=5, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.81\n",
      "Valid1 F1 Score : 0.54\n",
      "Valid2 F1 Score : 0.56\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_num_words,input_length = seq_len,output_dim = embedding_size))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 48, 64)            19264     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,027,713\n",
      "Trainable params: 1,027,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[F1_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      " - 49s - loss: 0.5648 - F1_score: 0.0000e+00 - val_loss: 0.4968 - val_F1_score: 0.0065\n",
      "Epoch 2/5\n",
      " - 49s - loss: 0.3961 - F1_score: 0.5033 - val_loss: 0.3893 - val_F1_score: 0.6402\n",
      "Epoch 3/5\n",
      " - 49s - loss: 0.2647 - F1_score: 0.8084 - val_loss: 0.3696 - val_F1_score: 0.6864\n",
      "Epoch 4/5\n",
      " - 49s - loss: 0.1841 - F1_score: 0.8791 - val_loss: 0.3910 - val_F1_score: 0.7005\n",
      "Epoch 5/5\n",
      " - 50s - loss: 0.1232 - F1_score: 0.9252 - val_loss: 0.4382 - val_F1_score: 0.6100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a498bb9d30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          validation_data=(X_valid1, y_valid1),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.96\n",
      "Valid1 F1 Score : 0.62\n",
      "Valid2 F1 Score : 0.65\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_classes = model.predict_classes(X_test)\n",
    "\n",
    "# submission = pd.read_csv('samplesubmission.csv')\n",
    "\n",
    "# submission.sentiment = test_classes.astype('int64')\n",
    "\n",
    "# submission.sentiment.dtype\n",
    "\n",
    "# submission.sentiment.value_counts(normalize=True)\n",
    "# Test Score- 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Hidden Layer output in SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "emd = K.function(inputs=[model.layers[0].input], \n",
    "                 outputs=[model.layers[0].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emm_train = emd([X_train])[0]\n",
    "emm_valid1 = emd([X_valid1])[0]\n",
    "emm_valid2 = emd([X_valid2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = emm_train.shape\n",
    "emm_train = emm_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = emm_valid1.shape\n",
    "emm_valid1 = emm_valid1.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = emm_valid2.shape\n",
    "emm_valid2 = emm_valid2.reshape((nsamples,nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.88\n",
      "Valid1 F1 Score : 0.65\n",
      "Valid2 F1 Score : 0.68\n"
     ]
    }
   ],
   "source": [
    "svm_classifier_linear = svm.SVC(kernel=\"linear\", class_weight=\"balanced\",C=0.05)\n",
    "\n",
    "svm_clf = svm_classifier_linear.fit(emm_train,y_train)\n",
    "\n",
    "train_pred = svm_clf.predict(emm_train)\n",
    "\n",
    "valid1_pred = svm_clf.predict(emm_valid1)\n",
    "\n",
    "valid2_pred = svm_clf.predict(emm_valid2)\n",
    "\n",
    "print('Train F1 Score :',round(f1_score(y_train,train_pred),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_pred),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim = max_num_words, \n",
    "                    input_length = seq_len, \n",
    "                    output_dim = embedding_size)) # Add an embedding layer which represents each unique token as a vector\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(LSTM(20, return_sequences=True,dropout=0.9)) # Add an LSTM layer\n",
    "model.add(LSTM(10, return_sequences=False))\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 20)            9680      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10)                1240      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,010,931\n",
      "Trainable params: 1,010,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=[F1_score])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 0.5993 - F1_score: 0.0167 - val_loss: 0.5546 - val_F1_score: 0.0000e+00\n",
      "Epoch 2/20\n",
      " - 2s - loss: 0.5561 - F1_score: 0.0000e+00 - val_loss: 0.5412 - val_F1_score: 0.0000e+00\n",
      "Epoch 3/20\n",
      " - 2s - loss: 0.5376 - F1_score: 0.0419 - val_loss: 0.5224 - val_F1_score: 0.0290\n",
      "Epoch 4/20\n",
      " - 2s - loss: 0.5150 - F1_score: 0.1635 - val_loss: 0.5000 - val_F1_score: 0.1791\n",
      "Epoch 5/20\n",
      " - 2s - loss: 0.4918 - F1_score: 0.3668 - val_loss: 0.4721 - val_F1_score: 0.3020\n",
      "Epoch 6/20\n",
      " - 2s - loss: 0.4419 - F1_score: 0.4994 - val_loss: 0.4382 - val_F1_score: 0.4555\n",
      "Epoch 7/20\n",
      " - 2s - loss: 0.4097 - F1_score: 0.5903 - val_loss: 0.4240 - val_F1_score: 0.5045\n",
      "Epoch 8/20\n",
      " - 2s - loss: 0.3905 - F1_score: 0.6474 - val_loss: 0.4211 - val_F1_score: 0.5215\n",
      "Epoch 9/20\n",
      " - 2s - loss: 0.3571 - F1_score: 0.6723 - val_loss: 0.4208 - val_F1_score: 0.5581\n",
      "Epoch 10/20\n",
      " - 2s - loss: 0.3459 - F1_score: 0.6883 - val_loss: 0.4188 - val_F1_score: 0.5975\n",
      "Epoch 11/20\n",
      " - 2s - loss: 0.3121 - F1_score: 0.7375 - val_loss: 0.4560 - val_F1_score: 0.5480\n",
      "Epoch 12/20\n",
      " - 2s - loss: 0.3095 - F1_score: 0.7228 - val_loss: 0.4285 - val_F1_score: 0.6315\n",
      "Epoch 13/20\n",
      " - 2s - loss: 0.2741 - F1_score: 0.7767 - val_loss: 0.4390 - val_F1_score: 0.6256\n",
      "Epoch 14/20\n",
      " - 3s - loss: 0.2680 - F1_score: 0.7825 - val_loss: 0.4420 - val_F1_score: 0.6399\n",
      "Epoch 15/20\n",
      " - 2s - loss: 0.2572 - F1_score: 0.7918 - val_loss: 0.4740 - val_F1_score: 0.5780\n",
      "Epoch 16/20\n",
      " - 3s - loss: 0.2519 - F1_score: 0.8094 - val_loss: 0.4942 - val_F1_score: 0.5599\n",
      "Epoch 17/20\n",
      " - 2s - loss: 0.2356 - F1_score: 0.8260 - val_loss: 0.4870 - val_F1_score: 0.5991\n",
      "Epoch 18/20\n",
      " - 3s - loss: 0.2294 - F1_score: 0.8176 - val_loss: 0.4912 - val_F1_score: 0.6158\n",
      "Epoch 19/20\n",
      " - 3s - loss: 0.2222 - F1_score: 0.8263 - val_loss: 0.5190 - val_F1_score: 0.5952\n",
      "Epoch 20/20\n",
      " - 2s - loss: 0.2035 - F1_score: 0.8478 - val_loss: 0.4895 - val_F1_score: 0.6289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4a0883cf8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          validation_data=(X_valid1, y_valid1),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.89\n",
      "Valid1 F1 Score : 0.63\n",
      "Valid2 F1 Score : 0.65\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_words,seq_len,embedding_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 150)           1500000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                17568     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,517,601\n",
      "Trainable params: 1,517,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_num_words, \n",
    "                    input_length = seq_len, \n",
    "                    output_dim = 150))\n",
    "model.add(GRU(32,dropout=0.8,recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/7\n",
      " - 5s - loss: 0.6023 - acc: 0.7048 - val_loss: 0.5335 - val_acc: 0.7222\n",
      "Epoch 2/7\n",
      " - 4s - loss: 0.4969 - acc: 0.7371 - val_loss: 0.4767 - val_acc: 0.7422\n",
      "Epoch 3/7\n",
      " - 4s - loss: 0.4111 - acc: 0.8157 - val_loss: 0.4453 - val_acc: 0.7600\n",
      "Epoch 4/7\n",
      " - 4s - loss: 0.3267 - acc: 0.8633 - val_loss: 0.4612 - val_acc: 0.7711\n",
      "Epoch 5/7\n",
      " - 4s - loss: 0.2755 - acc: 0.8948 - val_loss: 0.5063 - val_acc: 0.7722\n",
      "Epoch 6/7\n",
      " - 4s - loss: 0.2357 - acc: 0.9105 - val_loss: 0.5408 - val_acc: 0.7689\n",
      "Epoch 7/7\n",
      " - 3s - loss: 0.2066 - acc: 0.9219 - val_loss: 0.5749 - val_acc: 0.7611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4a0a74be0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=7,\n",
    "          validation_data=(X_valid1, y_valid1),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.91\n",
      "Valid1 F1 Score : 0.56\n",
      "Valid2 F1 Score : 0.61\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pretrained Embeddings from GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# f = open('glove.6B.100d.txt',encoding='utf8')\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n",
    "# for word, i in word_index.items():\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = Embedding(len(word_index) + 1,\n",
    "#                             embedding_size,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=seq_len,\n",
    "#                             trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(len(word_index) + 1,\n",
    "#                             embedding_size,\n",
    "#                             weights=[embedding_matrix],\n",
    "#                             input_length=seq_len,\n",
    "#                             trainable=False))\n",
    "# model.add(Conv1D(64, 3, activation='relu'))\n",
    "# model.add(Conv1D(64, 3, activation='relu'))\n",
    "# model.add(GlobalMaxPool1D())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=[F1_score])\n",
    "# model.fit(X_train, y_train,\n",
    "#           batch_size=64,\n",
    "#           epochs=5,\n",
    "#           validation_data=(X_valid1, y_valid1),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_classes = model.predict_classes(X_train)\n",
    "# valid1_classes = model.predict_classes(X_valid1)\n",
    "# valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "# print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "# print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer 1 GRU(64,dropout=0.4,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.5,return_sequences=False)\n",
    "batch_size=64, epochs=10,\n",
    "Train F1 Score : 0.99\n",
    "Valid1 F1 Score : 0.62\n",
    "Valid2 F1 Score : 0.63\n",
    "<br><br>\n",
    "layer 1 GRU(64,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.5,return_sequences=False)\n",
    "batch_size=64, epochs=10,\n",
    "Train F1 Score : 0.9\n",
    "Valid1 F1 Score : 0.62\n",
    "Valid2 F1 Score : 0.65\n",
    "<br><br>\n",
    "layer 1 GRU(64,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.9,return_sequences=False)\n",
    "batch_size=64, epochs=10,\n",
    "Train F1 Score : 0.85\n",
    "Valid1 F1 Score : 0.6\n",
    "Valid2 F1 Score : 0.61\n",
    "<br><br>\n",
    "layer 1 GRU(64,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.9,return_sequences=False)\n",
    "batch_size=16, epochs=10,\n",
    "Train F1 Score : 0.9\n",
    "Valid1 F1 Score : 0.67\n",
    "Valid2 F1 Score : 0.69\n",
    "\n",
    "clean_doc\n",
    "Train F1 Score : 0.89\n",
    "Valid1 F1 Score : 0.65\n",
    "Valid2 F1 Score : 0.67\n",
    "<br><br>\n",
    "layer 1 GRU(64,dropout=0.5,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.5,return_sequences=False)\n",
    "batch_size=16, epochs=10,\n",
    "Train F1 Score : 0.99\n",
    "Valid1 F1 Score : 0.63\n",
    "Valid2 F1 Score : 0.64\n",
    "<br><br>\n",
    "layer 1 GRU(32,dropout=0.5,return_sequences=True)\n",
    "layer 2 GRU(16,dropout=0.5,return_sequences=False)\n",
    "batch_size=16, epochs=10,\n",
    "Train F1 Score : 0.99\n",
    "Valid1 F1 Score : 0.64\n",
    "Valid2 F1 Score : 0.65\n",
    "<br><br>\n",
    "layer 1 GRU(32,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(16,dropout=0.9,return_sequences=False)\n",
    "batch_size=16, epochs=10,\n",
    "Train F1 Score : 0.88\n",
    "Valid1 F1 Score : 0.65\n",
    "Valid2 F1 Score : 0.65\n",
    "<br><br>\n",
    "layer 1 GRU(32,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(16,dropout=0.9,return_sequences=False)\n",
    "batch_size=16, epochs=15,\n",
    "Train F1 Score : 0.92\n",
    "Valid1 F1 Score : 0.63\n",
    "Valid2 F1 Score : 0.65\n",
    "<br><br>\n",
    "layer 1 GRU(32,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(16,dropout=0.9,return_sequences=False)\n",
    "batch_size=16, epochs=10,\n",
    "Train F1 Score : 0.89\n",
    "Valid1 F1 Score : 0.62\n",
    "Valid2 F1 Score : 0.67\n",
    "<br><br>\n",
    "layer 1 GRU(64,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.9,return_sequences=False)\n",
    "batch_size=16, epochs=10,\n",
    "Train F1 Score : 0.9\n",
    "Valid1 F1 Score : 0.61\n",
    "Valid2 F1 Score : 0.64\n",
    "<br><br>\n",
    "layer 1 GRU(64,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(32,dropout=0.9,return_sequences=False)\n",
    "batch_size=8, epochs=10,\n",
    "Train F1 Score : 0.91\n",
    "Valid1 F1 Score : 0.63\n",
    "Valid2 F1 Score : 0.67\n",
    "<br><br>\n",
    "layer 1 GRU(32,dropout=0.9,return_sequences=True)\n",
    "layer 2 GRU(16,dropout=0.9,return_sequences=False)\n",
    "batch_size=16, epochs=15,\n",
    "Train F1 Score : 0.9\n",
    "Valid1 F1 Score : 0.69\n",
    "Valid2 F1 Score : 0.7\n",
    "TEST SCORE : 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 32)          12768     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 16)                2352      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,015,137\n",
      "Trainable params: 1,015,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_GRU=Sequential()\n",
    "model_GRU.add(Embedding(max_num_words,100,mask_zero=True))\n",
    "model_GRU.add(GRU(32,dropout=0.9,return_sequences=True))\n",
    "model_GRU.add(GRU(16,dropout=0.9,return_sequences=False))\n",
    "model_GRU.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model_GRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/12\n",
      " - 11s - loss: 0.6294 - F1_score: 0.0403 - val_loss: 0.5973 - val_F1_score: 0.0000e+00\n",
      "Epoch 2/12\n",
      " - 9s - loss: 0.5969 - F1_score: 0.0069 - val_loss: 0.5834 - val_F1_score: 0.0000e+00\n",
      "Epoch 3/12\n",
      " - 10s - loss: 0.5752 - F1_score: 0.0294 - val_loss: 0.5458 - val_F1_score: 0.0000e+00\n",
      "Epoch 4/12\n",
      " - 10s - loss: 0.5146 - F1_score: 0.2878 - val_loss: 0.4352 - val_F1_score: 0.3906\n",
      "Epoch 5/12\n",
      " - 11s - loss: 0.4490 - F1_score: 0.4939 - val_loss: 0.4176 - val_F1_score: 0.6046\n",
      "Epoch 6/12\n",
      " - 11s - loss: 0.3958 - F1_score: 0.6128 - val_loss: 0.4149 - val_F1_score: 0.5545\n",
      "Epoch 7/12\n",
      " - 11s - loss: 0.3642 - F1_score: 0.6509 - val_loss: 0.4308 - val_F1_score: 0.5749\n",
      "Epoch 8/12\n",
      " - 11s - loss: 0.3503 - F1_score: 0.6972 - val_loss: 0.4676 - val_F1_score: 0.5353\n",
      "Epoch 9/12\n",
      " - 10s - loss: 0.3372 - F1_score: 0.6756 - val_loss: 0.4761 - val_F1_score: 0.5470\n",
      "Epoch 10/12\n",
      " - 11s - loss: 0.2926 - F1_score: 0.7563 - val_loss: 0.5310 - val_F1_score: 0.5381\n",
      "Epoch 11/12\n",
      " - 11s - loss: 0.2771 - F1_score: 0.7513 - val_loss: 0.5404 - val_F1_score: 0.5423\n",
      "Epoch 12/12\n",
      " - 12s - loss: 0.2651 - F1_score: 0.7693 - val_loss: 0.5402 - val_F1_score: 0.5654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4b3057470>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=[F1_score])\n",
    "model_GRU.fit(X_train, y_train,batch_size=16,\n",
    "                            epochs=12,\n",
    "                            validation_data=(X_valid1, y_valid1),\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model_GRU.predict_classes(X_train)\n",
    "valid1_classes = model_GRU.predict_classes(X_valid1)\n",
    "valid2_classes = model_GRU.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.9\n",
      "Valid1 F1 Score : 0.61\n",
      "Valid2 F1 Score : 0.62\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_classes = model_GRU.predict_classes(X_test)\n",
    "# # Train F1 Score : 0.9\n",
    "# # Valid1 F1 Score : 0.69\n",
    "# # Valid2 F1 Score : 0.7\n",
    "# Test score: 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = pd.read_csv('samplesubmission.csv')\n",
    "\n",
    "# submission.sentiment = test_classes.astype('int64')\n",
    "\n",
    "# submission.sentiment.value_counts(normalize=True)\n",
    "\n",
    "# submission.to_csv('submission7.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_LSTM.add(LSTM(64,dropout=0.4,return_sequences=True))\n",
    "model_LSTM.add(LSTM(32,dropout=0.5,return_sequences=False))\n",
    "batch_size=16,epochs=10\n",
    "Train F1 Score : 0.98\n",
    "Valid1 F1 Score : 0.57\n",
    "Valid2 F1 Score : 0.6\n",
    "<br><br>\n",
    "model_LSTM.add(LSTM(64,dropout=0.9,return_sequences=True))\n",
    "model_LSTM.add(LSTM(32,dropout=0.9,return_sequences=False))\n",
    "batch_size=16,epochs=10\n",
    "Train F1 Score : 0.88\n",
    "Valid1 F1 Score : 0.63\n",
    "Valid2 F1 Score : 0.67\n",
    "<br><br>\n",
    "model_LSTM.add(LSTM(64,dropout=0.9,return_sequences=True))\n",
    "model_LSTM.add(LSTM(32,dropout=0.9,return_sequences=False))\n",
    "batch_size=8,epochs=10\n",
    "Train F1 Score : 0.88\n",
    "Valid1 F1 Score : 0.59\n",
    "Valid2 F1 Score : 0.64\n",
    "<br><br>\n",
    "model_LSTM.add(LSTM(32,dropout=0.9,return_sequences=True))\n",
    "model_LSTM.add(LSTM(16,dropout=0.9,return_sequences=False))\n",
    "batch_size=16,epochs=10\n",
    "Train F1 Score : 0.86\n",
    "Valid1 F1 Score : 0.62\n",
    "Valid2 F1 Score : 0.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,017,057\n",
      "Trainable params: 1,017,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_LSTM=Sequential()\n",
    "model_LSTM.add(Embedding(max_num_words,100,mask_zero=True))\n",
    "model_LSTM.add(LSTM(32,dropout=0.9,return_sequences=False))\n",
    "model_LSTM.add(Dense(1,activation='sigmoid'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/10\n",
      " - 9s - loss: 0.6152 - F1_score: 0.0178 - val_loss: 0.5471 - val_F1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.4929 - F1_score: 0.2367 - val_loss: 0.4389 - val_F1_score: 0.4361\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.4097 - F1_score: 0.5728 - val_loss: 0.4037 - val_F1_score: 0.5188\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.3588 - F1_score: 0.6667 - val_loss: 0.3880 - val_F1_score: 0.5933\n",
      "Epoch 5/10\n",
      " - 7s - loss: 0.3204 - F1_score: 0.6795 - val_loss: 0.3992 - val_F1_score: 0.5954\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.2927 - F1_score: 0.7313 - val_loss: 0.3996 - val_F1_score: 0.6122\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.2571 - F1_score: 0.7667 - val_loss: 0.4283 - val_F1_score: 0.5702\n",
      "Epoch 8/10\n",
      " - 9s - loss: 0.2268 - F1_score: 0.8081 - val_loss: 0.4346 - val_F1_score: 0.6261\n",
      "Epoch 9/10\n",
      " - 9s - loss: 0.2024 - F1_score: 0.8307 - val_loss: 0.4750 - val_F1_score: 0.5894\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.1811 - F1_score: 0.8377 - val_loss: 0.4698 - val_F1_score: 0.6218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4bd6f6f98>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=[F1_score])\n",
    "model_LSTM.fit(X_train, y_train,batch_size=16,\n",
    "                            epochs=10,\n",
    "                            validation_data=(X_valid1, y_valid1),\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model_LSTM.predict_classes(X_train)\n",
    "valid1_classes = model_LSTM.predict_classes(X_valid1)\n",
    "valid2_classes = model_LSTM.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.92\n",
      "Valid1 F1 Score : 0.67\n",
      "Valid2 F1 Score : 0.67\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim = max_num_words, \n",
    "                    input_length = seq_len, \n",
    "                    output_dim = embedding_size)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(LSTM(25, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "adam = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.5647 - F1_score: 0.0645 - val_loss: 0.5088 - val_F1_score: 0.3051\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.3980 - F1_score: 0.6086 - val_loss: 0.4320 - val_F1_score: 0.5825\n",
      "Epoch 3/10\n",
      " - 7s - loss: 0.2517 - F1_score: 0.8018 - val_loss: 0.4739 - val_F1_score: 0.5784\n",
      "Epoch 4/10\n",
      " - 7s - loss: 0.1827 - F1_score: 0.8730 - val_loss: 0.5672 - val_F1_score: 0.5299\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.1335 - F1_score: 0.9188 - val_loss: 0.5662 - val_F1_score: 0.5680\n",
      "Epoch 6/10\n",
      " - 7s - loss: 0.1065 - F1_score: 0.9358 - val_loss: 0.6826 - val_F1_score: 0.5747\n",
      "Epoch 7/10\n",
      " - 7s - loss: 0.0885 - F1_score: 0.9457 - val_loss: 0.6629 - val_F1_score: 0.5843\n",
      "Epoch 8/10\n",
      " - 7s - loss: 0.0728 - F1_score: 0.9553 - val_loss: 0.7831 - val_F1_score: 0.5528\n",
      "Epoch 9/10\n",
      " - 8s - loss: 0.0772 - F1_score: 0.9495 - val_loss: 0.7904 - val_F1_score: 0.5370\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.0595 - F1_score: 0.9596 - val_loss: 0.7822 - val_F1_score: 0.5910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4be3aa588>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mention the optimizer, Loss function and metrics to be computed\n",
    "model.compile(optimizer=adam,                  # 'Adam' is a variant of gradient descent technique\n",
    "              loss='binary_crossentropy', # categorical_crossentropy for multi-class classification\n",
    "              metrics=[F1_score])            # These metrics are computed for evaluating and stored in history\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid1, y_valid1),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = model.predict_classes(X_train)\n",
    "valid1_classes = model.predict_classes(X_valid1)\n",
    "valid2_classes = model.predict_classes(X_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score : 0.98\n",
      "Valid1 F1 Score : 0.61\n",
      "Valid2 F1 Score : 0.63\n"
     ]
    }
   ],
   "source": [
    "print('Train F1 Score :',round(f1_score(y_train,train_classes),2))\n",
    "print('Valid1 F1 Score :',round(f1_score(y_valid1,valid1_classes),2))\n",
    "print('Valid2 F1 Score :',round(f1_score(y_valid2,valid2_classes),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
